{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcmontb/email-automation/blob/main/Prompting_capabilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7f6058-d0d1-44af-b43a-cb4b06df03d8",
      "metadata": {
        "id": "5d7f6058-d0d1-44af-b43a-cb4b06df03d8"
      },
      "source": [
        "# Prompting Capabilities\n",
        "\n",
        "When you first start using Mistral models, your first interaction will revolve around prompts. The art of crafting effective prompts is essential for generating desirable responses from Mistral models or other LLMs. This guide will walk you through example prompts showing four different prompting capabilities.\n",
        "\n",
        "- Classification\n",
        "- Summarization\n",
        "- Personalization\n",
        "- Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa37d97-10e4-425d-b852-15bd043662b9",
      "metadata": {
        "id": "daa37d97-10e4-425d-b852-15bd043662b9",
        "outputId": "9f08fd02-3e8f-4e7b-e3fb-4b0949dda30f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistralai==0.0.12\n",
            "  Downloading mistralai-0.0.12-py3-none-any.whl (14 kB)\n",
            "Collecting httpx<0.26.0,>=0.25.2 (from mistralai==0.0.12)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.10 (from mistralai==0.0.12)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from mistralai==0.0.12) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.26.0,>=0.25.2->mistralai==0.0.12)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.0.12) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.0.12) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai==0.0.12) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.26.0,>=0.25.2->mistralai==0.0.12) (1.2.1)\n",
            "Installing collected packages: orjson, h11, httpcore, httpx, mistralai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.25.2 mistralai-0.0.12 orjson-3.10.6\n"
          ]
        }
      ],
      "source": [
        "! pip install mistralai==0.0.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae93e75a-fd20-4ca6-9eff-bef7d44ab3a9",
      "metadata": {
        "id": "ae93e75a-fd20-4ca6-9eff-bef7d44ab3a9"
      },
      "outputs": [],
      "source": [
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ff9378-c059-4063-a8a0-9ec1d1186954",
      "metadata": {
        "id": "99ff9378-c059-4063-a8a0-9ec1d1186954"
      },
      "outputs": [],
      "source": [
        "api_key = \"iNJundeHHhDKD0IA5yB1lvsQTR7h2gn6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e030d9c2-1ecb-4bf0-864d-9f96b41bd016",
      "metadata": {
        "id": "e030d9c2-1ecb-4bf0-864d-9f96b41bd016"
      },
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-medium\"):\n",
        "    client = MistralClient(api_key=api_key)\n",
        "    messages = [\n",
        "        ChatMessage(role=\"user\", content=user_message)\n",
        "    ]\n",
        "    chat_response = client.chat(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98a29ef-4764-40b7-aed8-0e5d0502f985",
      "metadata": {
        "id": "b98a29ef-4764-40b7-aed8-0e5d0502f985"
      },
      "source": [
        "## Classification\n",
        "Mistral models can easily categorize text into distinct classes. In this example prompt, we can define a list of predefined categories and ask Mistral models to classify user inquiry.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b208e1",
      "metadata": {
        "id": "f8b208e1"
      },
      "outputs": [],
      "source": [
        "def user_message(inquiry):\n",
        "    user_message = (\n",
        "        f\"\"\"\n",
        "        You are a venture capital analyst bot. Your task is to assess investment opportunities described after <<<>>> and categorize them into one of the following predefined categories:\n",
        "\n",
        "        dev tool\n",
        "        energy\n",
        "        SaaS\n",
        "        logistics\n",
        "        healthtech\n",
        "        cybersecurity\n",
        "        pet services\n",
        "        HR tech\n",
        "        edtech\n",
        "        fintech\n",
        "        biotech\n",
        "        agritech\n",
        "        proptech\n",
        "\n",
        "        If the text doesn't fit into any of the above categories, classify it as:\n",
        "        other\n",
        "\n",
        "        You will only respond with the category. Do not include the word \"Category\". Do not provide explanations or notes.\n",
        "\n",
        "        ####\n",
        "        Here are some examples:\n",
        "\n",
        "        Inquiry: We are building a tool that autocompletes code in the IDE.\n",
        "        Category: dev tool\n",
        "        Inquiry: We are building a new type of battery cell for the high end auto market.\n",
        "        Category: energy\n",
        "        Inquiry: A SaaS tool that helps restaurant owners categorize invoices which are scanned using OCR.\n",
        "        Category: SaaS\n",
        "        Inquiry: Our platform leverages AI to optimize supply chain logistics for e-commerce businesses.\n",
        "        Category: logistics\n",
        "        Inquiry: Developing a mobile app that offers personalized mental health support using CBT techniques.\n",
        "        Category: healthtech\n",
        "        Inquiry: We are developing an AI-driven tool to enhance cybersecurity for financial institutions.\n",
        "        Category: cybersecurity\n",
        "        Inquiry: A new app that connects pet owners with local veterinarians on-demand.\n",
        "        Category: pet services\n",
        "        Inquiry: We are developing a new platform to streamline HR processes for large enterprises.\n",
        "        Category: HR tech\n",
        "        Inquiry: An online platform to help students prepare for standardized tests using personalized learning algorithms.\n",
        "        Category: edtech\n",
        "        Inquiry: We are creating a mobile banking app that offers real-time expense tracking and budgeting tools.\n",
        "        Category: fintech\n",
        "        Inquiry: Our company is developing a new gene editing technology for personalized medicine.\n",
        "        Category: biotech\n",
        "        Inquiry: We are building an IoT-based solution to monitor crop health and optimize irrigation in real-time.\n",
        "        Category: agritech\n",
        "        Inquiry: Developing a digital marketplace to connect property buyers with real estate agents using VR tours.\n",
        "        Category: proptech\n",
        "\n",
        "        ###\n",
        "\n",
        "        <<<\n",
        "        Inquiry: {inquiry}\n",
        "        >>>\n",
        "        \"\"\"\n",
        "    )\n",
        "    return user_message"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8863460a-7670-4b6f-b511-cf1df7693cfa",
      "metadata": {
        "id": "8863460a-7670-4b6f-b511-cf1df7693cfa"
      },
      "source": [
        "### Strategies we used:\n",
        "\n",
        "- **Few shot learning**: Few-shot learning or in-context learning is when we give a few examples in the prompts, and the LLM can generate corresponding output based on the example demonstrations. Few-shot learning can often improve model performance especially when the task is difficult or when we want the model to respond in a specific manner.\n",
        "- **Delimiter**: Delimiters like ### <<< >>> specify the boundary between different sections of the text. In our example, we used ### to indicate examples and <<<>>> to indicate customer inquiry.\n",
        "- **Role playing**: Providing LLM a role (e.g., \"You are a bank customer service bot.\") adds personal context to the model and often leads to better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8a83cc-31e6-4d4b-b252-93d9133ecbf5",
      "metadata": {
        "id": "9d8a83cc-31e6-4d4b-b252-93d9133ecbf5",
        "outputId": "640e59cc-4d6b-4a91-fea5-2940d0dc23dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HR tech\n"
          ]
        }
      ],
      "source": [
        "print(run_mistral(user_message(\n",
        "\"We provide an all-in-one software solution for managing employee data, payroll, and benefits.\"\t)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6eca06b-7b1d-4663-9a68-2a23116f8c6e",
      "metadata": {
        "id": "d6eca06b-7b1d-4663-9a68-2a23116f8c6e",
        "outputId": "bbcd08b1-2677-4d96-c7b6-b3c41471a204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "other\n"
          ]
        }
      ],
      "source": [
        "print(run_mistral(user_message(\"What's the weather today?\")))"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:light"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}